{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**Build your own Transformer from scratch using Pytorch**](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src_vocab_size (Tamaño del Vocabulario de la Fuente): Este parámetro define el tamaño del vocabulario para el idioma de entrada (fuente). Un tamaño de vocabulario de 5000 significa que el modelo usará 5000 palabras/tokens únicos para representar el texto en el idioma de entrada.\n",
    "\n",
    "tgt_vocab_size (Tamaño del Vocabulario del Objetivo): Similar al src_vocab_size, pero para el idioma de salida (objetivo). Define cuántas palabras/tokens únicos se usarán para representar el texto en el idioma de salida.\n",
    "\n",
    "num_layers (Número de Capas): Este parámetro define cuántas capas de codificadores y decodificadores tiene el Transformer. Cada capa aprende diferentes aspectos de los datos. Aquí, 6 capas significan que hay 6 capas de codificadores y 6 capas de decodificadores en el modelo.\n",
    "\n",
    "\n",
    "max_seq_length (Longitud Máxima de Secuencia): Define la longitud máxima de las secuencias de entrada y salida que el modelo puede manejar. Un valor de 100 significa que el modelo puede procesar secuencias de hasta 100 tokens.\n",
    "\n",
    "dropout: Es una técnica para prevenir el sobreajuste en redes neuronales. El valor de 0.1 indica que hay una probabilidad del 10% de que cualquier neurona se \"apague\" durante el entrenamiento, lo que ayuda a que el modelo sea más robusto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
    "![Multi-Heads Attentions](resources\\multi_head_attention.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"The MultiHeadAttention code initializes the module with input parameters and linear transformation layers. It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads. The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence.    \n",
    "    \"\"\"\n",
    "    def __init__(self, embedd_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        embedd_dim (int): Dimension del vector de embeddings para cada token.  Este  valor\n",
    "        también determina el tamaño de las capas de atención y feed-forward en el modelo.\n",
    "\n",
    "        num_heads (int): El número de cabezas en el Multi-Head Attention. Este valor debe \n",
    "        ser un divisor de 'embedd_dim' para permitir una distribución equitativa de las dimensiones a cada cabeza.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embedd_dim % num_heads == 0, \"embedd_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embedd_dim = embedd_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # head_dim represent the dimension that each head will receive\n",
    "        self.head_dim = embedd_dim // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(embedd_dim, embedd_dim, bias=False)\n",
    "        self.W_key = nn.Linear(embedd_dim, embedd_dim, bias=False)\n",
    "        self.W_value = nn.Linear(embedd_dim, embedd_dim, bias=False)\n",
    "        self.W_out = nn.Linear(embedd_dim, embedd_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "\n",
    "        # NOTE: torch.matmul perform a matrix multiplication in the last two dimensions\n",
    "        attn_scores = torch.matmul(\n",
    "            Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # after matmul, attn_scores has shape (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask is a tensor of shape (batch_size, 1, seq_length, seq_length)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to attn_scores along the last dimension. For a specific element\n",
    "        # (i, j, k, :), where i is the batch index, j is the head index, and k\n",
    "        # is a specific sequence position. These probabilities in the last dimension indicate\n",
    "        # the relative importance of each sequence position in calculating the attended\n",
    "        # representation for the  position k in this example.\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # (batch_size, num_heads, seq_length, seq_length) X\n",
    "        # (batch_size, num_heads, seq_length, head_dim)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        # after matmul output has shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, embedd_dim = x.size()\n",
    "        # return a tensor of shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        # return a tensor of shape (batch_size, seq_length, embedd_dim)\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embedd_dim)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_query(Q))\n",
    "        K = self.split_heads(self.W_key(K))\n",
    "        V = self.split_heads(self.W_value(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_out(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\" This process enables the model to consider the position of input elements while making predictions through the use of RELU.\"\"\"\n",
    "\n",
    "    def __init__(self, embedd_dim, feed_forward_dim):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedd_dim, feed_forward_dim)\n",
    "        self.fc2 = nn.Linear(feed_forward_dim, embedd_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, embedd_dim, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embedd_dim)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedd_dim, 2).float() * -(math.log(10000.0) / embedd_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Encoder and Decoder layers\n",
    "\n",
    "The **EncoderLayer** class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.\n",
    "\n",
    "![Encoder](resources\\encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedd_dim, num_heads, feed_forward_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(embedd_dim, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(embedd_dim, feed_forward_dim)\n",
    "        self.norm1 = nn.LayerNorm(embedd_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedd_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DecoderLayer** initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention and cross-attention, a PositionWiseFeedForward module, three layer normalization modules, and a dropout layer.\n",
    "\n",
    "![decoder](resources\\decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedd_dim, num_heads, feed_forward_dim, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(embedd_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(embedd_dim, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(embedd_dim, feed_forward_dim)\n",
    "        self.norm1 = nn.LayerNorm(embedd_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedd_dim)\n",
    "        self.norm3 = nn.LayerNorm(embedd_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # Calculate the masked self-attention output and add it to the input tensor, \n",
    "        # followed by dropout and layer normalization.\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Compute the cross-attention output between the decoder and encoder outputs, and\n",
    "        # add it to the normalized masked self-attention output, followed by dropout and layer normalization.\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Calculate the position-wise feed-forward output and combine it with the normalized cross-attention output, followed by dropout and layer normalization.\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine Encoder and Decoder layers to create the complete Transformer model\n",
    "1. Generate source and target masks using the generate_mask method.\n",
    "2. Compute source and target embeddings, and apply positional encoding and dropout.\n",
    "3. Process the source sequence through encoder layers, updating the enc_output tensor.\n",
    "4. Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.\n",
    "\n",
    "5. Apply the linear projection layer to the decoder output, obtaining output logits.               \n",
    "    \n",
    "![Attention Is All You Need](resources\\trasformer_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer model for sequence-to-sequence tasks, based on the architecture \n",
    "    introduced by Vaswani et al. in the paper \"Attention Is All You Need\". \n",
    "\n",
    "    This implementation includes the encoder and decoder parts of the Transformer, \n",
    "    utilizing multi-head self-attention and position-wise feed-forward networks.\n",
    "\n",
    "    Attributes:\n",
    "        src_vocab_size (int): Size of the source vocabulary.\n",
    "        tgt_vocab_size (int): Size of the target vocabulary.\n",
    "        embedd_dim (int): Dimensionality of the embedding space.\n",
    "        num_heads (int): Number of attention heads in the multi-head attention layers.\n",
    "        num_layers (int): Number of encoder and decoder layers.\n",
    "        feed_forward_dim (int): Dimensionality of the feed-forward networks.\n",
    "        max_seq_length (int): Maximum sequence length that this model can process.\n",
    "        dropout (float): Dropout rate for regularization in the transformer layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedd_dim, num_heads, num_layers, feed_forward_dim, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, embedd_dim)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embedd_dim)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            embedd_dim, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(\n",
    "            embedd_dim, num_heads, feed_forward_dim, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(\n",
    "            embedd_dim, num_heads, feed_forward_dim, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(embedd_dim, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens.\n",
    "        \"\"\"\n",
    "        # unsqueeze para agregar dos dimensiones adicionales y  alinear la máscara con las dimensiones esperadas (batch_size, 1, 1, seq_length).\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (\n",
    "            1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(\n",
    "            self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(\n",
    "            self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example of training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.482845306396484\n",
      "Epoch: 2, Loss: 8.427691459655762\n",
      "Epoch: 3, Loss: 8.35286808013916\n",
      "Epoch: 4, Loss: 8.276569366455078\n",
      "Epoch: 5, Loss: 8.194737434387207\n",
      "Epoch: 6, Loss: 8.115641593933105\n",
      "Epoch: 7, Loss: 8.036117553710938\n",
      "Epoch: 8, Loss: 7.949494361877441\n",
      "Epoch: 9, Loss: 7.86048698425293\n",
      "Epoch: 10, Loss: 7.777308464050293\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(),\n",
    "                       lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer.to(device)\n",
    "src_data = src_data.to(device)\n",
    "tgt_data = tgt_data.to(device)\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size),\n",
    "                     tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
